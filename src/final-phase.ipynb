{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astromanish/NMT/blob/main/NMT_Final_Phase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1VPqyMPen34"
      },
      "source": [
        "# 1.Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pyUyWyterCC"
      },
      "source": [
        "This notebook contains the sequence to sequence model with attention mechanism for Hindi to English Neural Machine Translation using Pytorch. For encoder, a bi-directional GRU with 1 neural layer is used and the decoder uses attention layer and uni-directional GRU with 1 neural layer. The source and target language sentences are appended with start of sequence (\\<sos\\>) and end of sequence (\\<eos\\>) tokens. IndicNLP is used for tokenization of Hindi sentences and NLTK is used for tokenization of English sentences. AdamW optimizer and Cross Entropy Loss Function are used for computation of loss and to update the parameters of the model.\n",
        "The notebook is divided into the following sections:\n",
        "\n",
        "1. Introduction\n",
        "2. Installing the required packages\n",
        "3. Pre-processing data\n",
        "4. Building the Vocabulary\n",
        "5. Model Architecture\n",
        "6. Training the Model\n",
        "7. Testing the Model\n",
        "8. Generating Predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhteQdgti9jf"
      },
      "source": [
        "# 2. Installing the required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK2kj6qucJL3"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import torch\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "futxYnj4q_wY",
        "outputId": "caf51c85-c316-4dd1-b227-6c1d5a0a0b05"
      },
      "outputs": [],
      "source": [
        "#installing Indic NLP packages for hindi and english tokenizer\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ08lNgNrCgd"
      },
      "outputs": [],
      "source": [
        "INDIC_NLP_LIB_HOME = (\n",
        "    \"./indic_nlp_library/\"  # path to local git repo for Indic NLP library\n",
        ")\n",
        "INDIC_NLP_RESOURCES = (\n",
        "    \"./indic_nlp_resources/\"  # path to local git repo for Indic NLP Resources\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl2wzBelrI3O"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(r\"{}\".format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "\n",
        "loader.load()\n",
        "from indicnlp.tokenize import indic_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nKE4hMlWXVQ"
      },
      "source": [
        "# 3. Pre-processing data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6609FXusr0wf"
      },
      "source": [
        "In this section, the training data is prepared. The following steps are carried out for cleaning the data in train.csv file:\n",
        "\n",
        "1. All the English letters are converted to lower case.\n",
        "2. Punctuation marks and symbols like -, (, ), {, }, [, ], :, ,\\\", #, /, \\\\, ♪, \\=, ¶, ~ are removed from Hindi and English sentences. '\\&' is replaced with 'and'.\n",
        "3. If at the end of a Hindi sentence, a purn viram ('|') is not present then it is added.\n",
        "4. All the Devanagari numerals are replaced by Western Arabic Numerals, for example, \"१\" is replaced with \"1\".\n",
        "5. Hindi sentences containing any English word are removed from the train set. For example, sentences like \"मैंने तुमे School से हटवा दिया.\" were deleted.\n",
        "6. Multiple occurrrences of period(.), quoatation marks(\") and spaces are replaced with a single occurrence.\n",
        "7. Sentence with more than 70 words are removed from the train set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRg3kwmCWdCn",
        "outputId": "34004e2b-883b-42f7-fc19-d215548f272f"
      },
      "outputs": [],
      "source": [
        "train_set = []  # list to store pair of Hindi and English sentences\n",
        "i = 0\n",
        "with open(\"../data/raw_train.csv\", \"r\") as f:  # reading the train.csv file\n",
        "    csv_reader = csv.reader(f, delimiter=\",\")\n",
        "    for row in csv_reader:\n",
        "        flag = 0\n",
        "        if (\n",
        "            i == 0\n",
        "        ):  # To skip the column names from getting stored in the train data list.\n",
        "            i += 1\n",
        "            continue\n",
        "        for j in range(\n",
        "            65, 123\n",
        "        ):  # checking if the hindi sentence contains any English word\n",
        "            if chr(j) in row[1]:\n",
        "                flag = 1\n",
        "                break\n",
        "        if (\n",
        "            flag == 0\n",
        "        ):  # adding the sentence pair to train set only if Hindi sentence doesn't contain any English word\n",
        "            train_set.append(\n",
        "                [row[1], row[2].lower()]\n",
        "            )  # lower casing the english sentences while storing them in list\n",
        "train_set[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDT7i0fUWrfj"
      },
      "outputs": [],
      "source": [
        "# removing the punctuation, unnecessary spaces and replacing Devanagari numerals\n",
        "processing_dict = {\n",
        "    \"-\": \" \",\n",
        "    \"(\": \" \",\n",
        "    \")\": \" \",\n",
        "    \"{\": \" \",\n",
        "    \"}\": \" \",\n",
        "    \"[\": \" \",\n",
        "    \"]\": \" \",\n",
        "    \":\": \" \",\n",
        "    '\"': \" \",\n",
        "    \"\\&\": \" and \",\n",
        "    \"#\": \" \",\n",
        "    \"/\": \" \",\n",
        "    \"\\\\\": \" \",\n",
        "    \"♪\": \" \",\n",
        "    \"\\=\": \" \",\n",
        "    \"¶\": \" \",\n",
        "    \"~\": \" \",\n",
        "    \"  \": \" \",\n",
        "    \"%\": \" \",\n",
        "    \",\": \"\",\n",
        "    \"♫\": \" \",\n",
        "}\n",
        "for i in range(0, len(train_set)):\n",
        "    for j in range(0, 2):\n",
        "        for src, trg in processing_dict.items():\n",
        "            if (\n",
        "                src in train_set[i][j]\n",
        "            ):  # check if any character to be replaced is present in sentences\n",
        "                train_set[i][j] = train_set[i][j].replace(src, trg)\n",
        "\n",
        "        # multiple quotation marks are replaced by single quotation mark\n",
        "        train_set[i][0] = re.sub('\"+', \"\", train_set[i][0])\n",
        "        train_set[i][1] = re.sub('\"+', \"\", train_set[i][1])\n",
        "\n",
        "        # multiple occurrences of period are replaced with single occurrence\n",
        "        train_set[i][0] = train_set[i][0].replace(\"....\", \".\")\n",
        "        train_set[i][1] = train_set[i][1].replace(\"....\", \".\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"...\", \".\")\n",
        "        train_set[i][1] = train_set[i][1].replace(\"...\", \".\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"...\", \".\")\n",
        "        train_set[i][1] = train_set[i][1].replace(\"..\", \".\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"..\", \".\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\".\", \"|\")\n",
        "\n",
        "        # replacing the Devanagari Numerals with Western Arabic Numerals\n",
        "        train_set[i][0] = train_set[i][0].replace(\"०\", \"0\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"१\", \"1\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"२\", \"2\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"३\", \"3\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"४\", \"4\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"५\", \"5\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"६\", \"6\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"७\", \"7\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"८\", \"8\")\n",
        "        train_set[i][0] = train_set[i][0].replace(\"९\", \"9\")\n",
        "\n",
        "# storing the pre-processed data in a separate file\n",
        "with open(\"../data/pre_processed_train.csv\", \"w\") as f:\n",
        "    write = csv.writer(f)\n",
        "    write.writerow([\"hindi\", \"english\"])  # adding column names\n",
        "    write.writerows(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY0K9AGtoD6F",
        "outputId": "250494e3-b46b-4fb6-ed49-20ef7c07a608"
      },
      "outputs": [],
      "source": [
        "train_set[0:20]  # training data after pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85LfbAn6t-Lb"
      },
      "source": [
        "### Defining the Hindi and English Tokenizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Az6V2xwuCNR"
      },
      "source": [
        "For tokenization of Hindi sentences, indic_tokenize.trivial_tokenize() from IndicNLP is used. This tokenizer tokenizes the Hindi text on spaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6v6NDm8uncr"
      },
      "outputs": [],
      "source": [
        "# funtion to tokenize hindi text\n",
        "def hindi_tokenizer(text_in_hindi):\n",
        "    hindi_tokens = []\n",
        "    for token in indic_tokenize.trivial_tokenize(\n",
        "        text_in_hindi\n",
        "    ):  # trivial_tokenize of indicNLP is used for tokenization\n",
        "        hindi_tokens.append(token)\n",
        "    return hindi_tokens  # tokens of a sentence are returned as list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2n1l7keurmR"
      },
      "source": [
        "For tokenization of English sentences, Regexp tokenizer of NLTK is used. This tokenizer is selected for tokenization because while tokenizing it takes care of Apostophe symbol. For example, if the sentence is \"I'll be there\", then Regexp tokenizer will tokenize it as [\"I'll\", 'be', 'there'] whereas word_tokenizer() of NLTK will tokenize it as ['I', \"'ll\", 'be', 'there'].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09MQlAvHu4uQ"
      },
      "outputs": [],
      "source": [
        "regexp_tokenizer = RegexpTokenizer(\"[m\\w']+\")\n",
        "end_punctuation = [\".\", \"!\", \"?\"]\n",
        "\n",
        "\n",
        "def english_tokenizer(text_in_english):\n",
        "    english_tokens = []\n",
        "    for token in regexp_tokenizer.tokenize(text_in_english):\n",
        "        english_tokens.append(token)\n",
        "    # the Regexp tokenizer doesn't adds the punctuations like \". ! ?\" as tokens, so these punctuation marks are added as tokens to english sentences\n",
        "    if text_in_english[-1] in end_punctuation:\n",
        "        english_tokens.append(text_in_english[-1])\n",
        "    return english_tokens  # tokens of Hindi sentence are returned as list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQaD6AAfxwn2"
      },
      "source": [
        "Now, we need to define a maximum limit on the length of sentences which needs to be considered for training. For this, I used the maximum limit as 70 and all the sentence pairs having either Hindi or English sentence length greater than 70 were eliminated.\n",
        "\n",
        "Note: By length here I mean number of tokens and not number of characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4YW76_DxxES",
        "outputId": "1a1261c5-bd59-4d63-fe75-e45b28f1e7b2"
      },
      "outputs": [],
      "source": [
        "train_set_trimmed = (\n",
        "    []\n",
        ")  # list to store training data where sentence length is less than 70\n",
        "for pair in train_set:\n",
        "    if len(pair[0]) > 0 and len(pair[1]) > 0:\n",
        "        l1 = len(hindi_tokenizer(pair[0]))  # number of tokens in hindi sentence\n",
        "        l2 = len(english_tokenizer(pair[1]))  # number of tokens in english sentence\n",
        "        if (\n",
        "            1 <= l1 <= 70 and 1 <= l2 <= 70\n",
        "        ):  # check if length of both hindi and english sentence is less than 70\n",
        "            train_set_trimmed.append(\n",
        "                pair\n",
        "            )  # if length<70, then add the pair to trimmed train dataset\n",
        "# print(len(train_set_trimmed))\n",
        "print(train_set_trimmed[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUmseYCHwN1x"
      },
      "source": [
        "### Creating Train and validation set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "403mmNHzzBQu",
        "outputId": "7a5e5a0d-9a71-4189-cfad-a0480fc6fa43"
      },
      "outputs": [],
      "source": [
        "n = len(train_set_trimmed)  # length of trimmed datatset\n",
        "train_ratio = 0.90  # 90:10 ratio is used for train and validation/test data\n",
        "train_size = int(n * train_ratio)  # size of training data\n",
        "val_size = int(n - train_size)  # size of validation data\n",
        "\n",
        "# storing train and validation data in a separate list\n",
        "train_ds, val_ds = train_set_trimmed[:train_size], train_set_trimmed[train_size:]\n",
        "\n",
        "# length of train and validation data\n",
        "len(train_ds), len(val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67MgqAMz7dsP"
      },
      "outputs": [],
      "source": [
        "# saving the train and validation data in csv file\n",
        "\n",
        "with open(\"../data/final_train.csv\", \"w\") as f:\n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerows(train_ds)\n",
        "\n",
        "\n",
        "with open(\"../data/final_validation.csv\", \"w\") as f:\n",
        "    # using csv.writer method from CSV package\n",
        "    write = csv.writer(f)\n",
        "    write.writerows(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UenjvmTJGWci"
      },
      "source": [
        "# 4.Building the Vocabulary\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H87cvlTp0ZAw"
      },
      "source": [
        "To build the hindi and english vocabulary, first a list of tokens is generated using the tokenizer functions and then checked if the token already exists in the dictioanry. If the token is not present in the dictionary then it is assigned an index and added to the dictionary. Two dictionaries are maintained for both Hindi and English sentences. One dictionary maps the word to its corresponding index (word2index) and another maps the index to the corresponding word (index2word). One dictionary is maintained to keep the count of number of occurrences of each word in the corpus. This dictionary can help to limit the size of the vocabulary by keeping the most frequent words in vocabulary. However, in this code all the words present in corpus are taken in the vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUlxrP46Egqr"
      },
      "outputs": [],
      "source": [
        "sos_token = \"<sos>\"  # start of sequence token; appended at start of sentence\n",
        "eos_token = \"<eos>\"  # end of sequence token; appended at end of sentence\n",
        "unk_token = \"<unk>\"  # unknown token; used to represent a word if that word is not found in the dictionary\n",
        "pad_token = (\n",
        "    \"<pad>\"  # token for padding; used to make all sentences of equal length in a batch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### English Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDpAf8YdBMv5"
      },
      "outputs": [],
      "source": [
        "# dictionary to keep count of occurrence of each English word\n",
        "E_wordCount = {}\n",
        "\n",
        "# dictioanry to find the index for a word in English\n",
        "E_word2index = {sos_token: 0, eos_token: 1, unk_token: 2, pad_token: 3}\n",
        "\n",
        "# dictionary to find the English word for a particular index\n",
        "E_index2word = {0: sos_token, 1: eos_token, 2: unk_token, 3: pad_token}\n",
        "\n",
        "E_count = 4  # keeps count of number of words so far in English dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hindi Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dictionary to keep count of occurrence of each Hindi word\n",
        "H_wordCount = {}\n",
        "\n",
        "# dictioanry to find the index for a word in Hindi\n",
        "H_word2index = {sos_token: 0, eos_token: 1, unk_token: 2, pad_token: 3}\n",
        "\n",
        "# dictionary to find the Hindi word for a particular index\n",
        "H_index2word = {0: sos_token, 1: eos_token, 2: unk_token, 3: pad_token}\n",
        "\n",
        "H_count = 4  # keeps count of number of words so far in Hindi dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE2IwbD_1dAR"
      },
      "source": [
        "Defining functions to update the dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlIueCw2x_vr"
      },
      "outputs": [],
      "source": [
        "# function to add a word in English dictionary\n",
        "def E_updateDict(eng_sentence):\n",
        "    global E_count\n",
        "    tokens = english_tokenizer(eng_sentence)  # generating tokens for the given sentence\n",
        "    for token in tokens:\n",
        "        E_wordCount[token] = E_wordCount.get(token, 0) + 1\n",
        "        if (\n",
        "            token not in E_word2index.keys()\n",
        "        ):  # check if the token already exists in English dictionary\n",
        "            # if the token is not present in English dictionary then add it to word2index and index2word English dictionary\n",
        "            E_word2index[token] = E_count\n",
        "            E_index2word[E_count] = token\n",
        "            E_count += 1  # increasing the count of words in English vocabulary\n",
        "        else:\n",
        "            E_wordCount[\n",
        "                token\n",
        "            ] += 1  # if the token exists in dictionary then simply increase it's count of occurrence\n",
        "\n",
        "\n",
        "# function to add a word in Hindi dictionary\n",
        "def H_updateDict(hindi_sentence):\n",
        "    global H_count\n",
        "    tokens = hindi_tokenizer(hindi_sentence)  # generating tokens for the given sentence\n",
        "    for token in tokens:\n",
        "        H_wordCount[token] = H_wordCount.get(token, 0) + 1\n",
        "        if (\n",
        "            token not in H_word2index.keys()\n",
        "        ):  # check if the token already exists in Hindi dictionary\n",
        "            # if the token is not present in Hindi dictionary then add it to word2index and index2word Hindi dictionary\n",
        "            H_word2index[token] = H_count\n",
        "            H_index2word[H_count] = token\n",
        "            H_count += 1  # increasing the count of words in Hindi vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wATPy_6XFPfY"
      },
      "outputs": [],
      "source": [
        "# reading the training pairs to create hindi and english vocabulary\n",
        "for pair in train_ds:\n",
        "    H_updateDict(pair[0])  # updating hindi vocabulary\n",
        "    E_updateDict(pair[1])  # updating english vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_jXy7mJKH3T",
        "outputId": "ef741051-17e1-435b-8dde-cde78da0294c"
      },
      "outputs": [],
      "source": [
        "# number of words in hindi and english vocabulary\n",
        "print(H_count, E_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MS-pSY99XG5"
      },
      "source": [
        "# 5. Model Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDdp4cQ8728B"
      },
      "source": [
        "#### Defining the Encoder architecture\n",
        "\n",
        "For encoder a bidirectional GRU is used, where the forward RNN goes over the embedded sentence from left to right and the backward RNN goes over the embedded sentence from right to left. Due to bidirectional nature of encoder, we get two context vectors, one corresponding to each RNN. However, since the decoder used is unidirectional, so these context vectors are concatenated together through a linear layer and then the tanh activation function is applied.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ9CIX5mg4Zl"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, dropout_val):\n",
        "        # input_size is equal to hindi vocabulary size and embedding_size is equal to dimensions of embeddings\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout_val)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        # to make the GRU bidirectional we pass bidirectional=True parameter\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, bidirectional=True)\n",
        "        # fully connected linear layer\n",
        "        self.linear = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "    def forward(self, token_vec):\n",
        "        # token_vec is a vector of indices mapping a word to its index in the vocabulary. token_vec.shape()=[max_batch_length, batch size]\n",
        "\n",
        "        embedding = self.dropout(\n",
        "            self.embedding(token_vec)\n",
        "        )  # embedding is a 3D tensor of shape (seq length, batch_size, embedding_size)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = self.gru(\n",
        "            embedding\n",
        "        )  # the embedding is passed as input to the GRU\n",
        "        # encoder_outputs has dimensions [seq length, batch size, 2*hidden_size]\n",
        "        # encoder_hidden has dimensions [2, batch_size, hidden_size]\n",
        "        # in encoder_outputs and encoder_hidden we have 2 due to the bidirectional nature of GRU encoder. These are hidden states of both the forward RNN and backward RNN.\n",
        "\n",
        "        # concatinating the hidden states of both the layers using a linear layer and then applying tanh activation function\n",
        "        # encoder_hidden[-2,:,:] represents the hidden states from forward layer and encoder_hidden[-1,:,:] represents the hidden states from backward layer after the final time step\n",
        "        vec = torch.cat((encoder_hidden[-2, :, :], encoder_hidden[-1, :, :]), dim=1)\n",
        "        linear_layer_vec = self.linear(vec)\n",
        "        encoder_hidden = torch.tanh(linear_layer_vec)\n",
        "\n",
        "        return (\n",
        "            encoder_outputs,\n",
        "            encoder_hidden,\n",
        "        )  # returning the encoder output and hidden states\n",
        "        # encoder_outputs.shape=[seq_length, batch_size, 2*hidden_size], encoder_hidden.shape=[batch_size, hidden_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6sHOf8YOkBH"
      },
      "source": [
        "#### Defining the Attention Layer\n",
        "\n",
        "This layer returns a vector of attention score with sum of elements equal to 1. These scores basically tell the importance of a word in the input sentence for correct prediction of target sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrw51JVdKddw"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.attention_layer = nn.Linear((hidden_size * 2) + hidden_size, hidden_size)\n",
        "        self.weight_vector = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden has is a 2D tensor of dimensions [batch_size, hidden_size] and encoder_outputs has dimensions= [seq length, batch_size, 2*hidden_size]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        input_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # we need to calculate the energy between encoder states and previous decoder hidden state. For this, first we repeat the decoder hidden states input_len times\n",
        "        hidden = hidden.unsqueeze(1)\n",
        "        hidden = hidden.repeat(\n",
        "            1, input_len, 1\n",
        "        )  # hidden=(batch_size, seq_length, hidden_size)\n",
        "        encoder_outputs = encoder_outputs.permute(\n",
        "            1, 0, 2\n",
        "        )  # encoder_outputs=(batch_size, seq_length, 2*hidden_size)\n",
        "\n",
        "        # The encoder outputs and decoder hiddens states are concatenated using a linear layer and then tanh activation function is applied\n",
        "        new_vec = torch.cat(\n",
        "            (hidden, encoder_outputs), dim=2\n",
        "        )  # (batch_size, seq_length, 2*hidden_size+hidden_size)\n",
        "        a = self.attention_layer(new_vec)  # (batch_size, seq_length, hidden_size)\n",
        "        energy_values = torch.tanh(a)  # (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # computing the attention vector which tells the attention score for each encoder hidden state\n",
        "        attention_vector = self.weight_vector(\n",
        "            energy_values\n",
        "        )  # (batch_size, seq_length, 1)\n",
        "        attention_vector = attention_vector.squeeze(2)\n",
        "\n",
        "        # now the attention vector is passed through a softmax layer to ensure that each score is between 0 and 1 and sum of all the scores is 1.\n",
        "        return F.softmax(attention_vector, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKL2XeFr78nM"
      },
      "source": [
        "Definning the Decoder architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R77v_9F4B-_R"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, attention, embedding_size, hidden_size, output_size, dropout_val\n",
        "    ):\n",
        "        # here, embedding_size= dimensions of embedding as defined, hidden_size as defined and output_size=english vocabulary size\n",
        "        super().__init__()\n",
        "        self.attention = attention\n",
        "        self.output_size = output_size\n",
        "        self.dropout = nn.Dropout(dropout_val)\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.gru = nn.GRU((hidden_size * 2) + embedding_size, hidden_size)\n",
        "        self.linear_decoder = nn.Linear(\n",
        "            (hidden_size * 2) + hidden_size + embedding_size, output_size\n",
        "        )\n",
        "\n",
        "    def forward(self, token_vec, hidden, encoder_outputs):\n",
        "        # token_vec is one dimensional i.e. shape(token_vec) = (batch_size), hidden is a 2D vector of shape [batch_size, hidden_size]\n",
        "        # encoder_outputs have shape [seq len, batch_size, 2*hidden_size]\n",
        "        token_vec = token_vec.unsqueeze(0)  # adding one more dimension to the token_vec\n",
        "\n",
        "        embedded_input = self.embedding(\n",
        "            token_vec\n",
        "        )  # passing the input token_vec to embedding, [1, batch_size, embedding_size]\n",
        "        embedded_input = self.dropout(\n",
        "            embedded_input\n",
        "        )  # a 3D tensor of size [1, batch_size, embedding_size]\n",
        "\n",
        "        # previous decoder hidden and encoder states are passsed to the attention layer to get attention scores\n",
        "        a = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
        "        # a has dimension [batch_size, 1, seq length] i.e. a score is defined for each token in source sentence\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(\n",
        "            1, 0, 2\n",
        "        )  # (batch_size, seq len, 2*hidden_size)\n",
        "        weighted_vectors = torch.bmm(a, encoder_outputs).permute(\n",
        "            1, 0, 2\n",
        "        )  # (batch_size, 1, 2*hidden_size)\n",
        "        input_to_gru = torch.cat(\n",
        "            (embedded_input, weighted_vectors), dim=2\n",
        "        )  # vector with dimensions= [1, batch_size, 2*hidden_size + embedding_size]\n",
        "\n",
        "        decoder_output, hidden = self.gru(input_to_gru, hidden.unsqueeze(0))\n",
        "        embedded_input = embedded_input.squeeze(0)\n",
        "        decoder_output = decoder_output.squeeze(0)\n",
        "        weighted_vectors = weighted_vectors.squeeze(0)\n",
        "\n",
        "        # decoder output, weight vectors and embedded input are passed through a linear layer to predict the next token in target sentence\n",
        "        predicted_tokens = self.linear_decoder(\n",
        "            torch.cat((decoder_output, weighted_vectors, embedded_input), dim=1)\n",
        "        )\n",
        "        # prediction_tokens contains the predicted words and has dimensions [batch_size, output_size]\n",
        "        hidden = hidden.squeeze(0)\n",
        "\n",
        "        return predicted_tokens, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxgpaHKH8BIu"
      },
      "source": [
        "Defining the Seq2Seq class to define the model architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi6XfLVzJuQn"
      },
      "outputs": [],
      "source": [
        "class seq2seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(\n",
        "        self, input_token, target_token, teacher_force_ratio=0.5\n",
        "    ):  # teacher_force_ratio helps in preventing the model from overfitting and underfitting.\n",
        "        # teacher_force_ratio helps in deciding whether the next input word to the decoder will be actual/target word or the previous predicted word.\n",
        "        # input_token is tensor of [seq length, batch_size] shape and target_token has shape [target_length, batch-size]\n",
        "\n",
        "        batch_size = input_token.shape[1]\n",
        "        target_len = target_token.shape[0]\n",
        "        english_dict_size = self.decoder.output_size\n",
        "\n",
        "        # tensor to store decoder outputs, it is initially initialised to all zeroes\n",
        "        predicted_vector = torch.zeros(target_len, batch_size, english_dict_size).to(\n",
        "            self.device\n",
        "        )\n",
        "        encoder_outputs, hidden = self.encoder(input_token)\n",
        "        # encoder output stores all the hidden states in the input sequence both in forward and backward direction and hidden stores forward and backward hidden states after the final time step\n",
        "        token_vec = target_token[0, :]  # appending the <sos> token in prediction vector\n",
        "\n",
        "        for i in range(1, target_len):\n",
        "            output_token, hidden = self.decoder(\n",
        "                token_vec, hidden, encoder_outputs\n",
        "            )  # embedded input token, previous hidden states and encoder hidden states are passed to the decoder to obtain the prediction\n",
        "            predicted_vector[\n",
        "                i\n",
        "            ] = output_token  # output is appended to the prediction tokens\n",
        "\n",
        "            if (\n",
        "                random.random() < teacher_force_ratio\n",
        "            ):  # half of the times this will be true if teacher_force_ratio is 0.5\n",
        "                token_vec = target_token[\n",
        "                    i\n",
        "                ]  # in this case next input to the decoder is target/actual word\n",
        "            else:\n",
        "                token_vec = output_token.argmax(\n",
        "                    1\n",
        "                )  # in this case next input to the decoder is predicted word\n",
        "\n",
        "        return predicted_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLcoQOYF-P5P"
      },
      "source": [
        "# 6. Training the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "315KjUmA8MhA"
      },
      "source": [
        "Setting optimal hyperparameters for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ea-G1QeJ4iq"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "epochs = 25\n",
        "epoch_loss = 0.0  # training loss in each epoch\n",
        "layers = 1  # number of neural network layers in rnn\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = H_count\n",
        "output_size = E_count\n",
        "hidden_size = 512  # encoder and decoder have same hidden size\n",
        "embedding_size = 256  # encoder and decoder embedding size\n",
        "dropout = 0.5  # encoder and decoder dropout value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AskD18Gf8Yq5"
      },
      "source": [
        "**Preparing data for training the Model:** First, the data is sorted according to the length of Hindi sentences and then index vectors for these sentences are found using H_sentenceToTensor and E_sentenceToTensor functions. The logic behind sorting the training data is that in one batch we want sentences of similar lengths, so sorting helps us achieve that and padding is performed whereever necessary. To create batches of same length, I calculated the maximum length of sentence in a batch and stored this value in a dictionary with key as batch_id. After obtaining the maximum length for each batch, \"\\<pad\\>\" token was appended to the sentences whose length was less than the maximum length of sentence in that batch. After that, Dataloader is used to create batches of the required batch size. Each batch will have sentences of same length.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUuKTQ_rpMbe"
      },
      "outputs": [],
      "source": [
        "# sorting the training data according to length of hindi sentences\n",
        "train_ds.sort(key=lambda x: len(x[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZmj-IzHS4qx"
      },
      "outputs": [],
      "source": [
        "# finding the maximum length of sentence in a batch\n",
        "max_length_train = {}  # stores maximum length of sentences in training data\n",
        "batch_id = 1\n",
        "# computing maximum length for each batch of training data\n",
        "for i in range(0, len(train_ds), batch_size):\n",
        "    max_len = 0\n",
        "    for pair in train_ds[i : i + batch_size]:\n",
        "        E_maxlength = 0\n",
        "        H_maxlength = 0\n",
        "        for token in hindi_tokenizer(pair[0]):\n",
        "            H_maxlength += 1\n",
        "        for token in english_tokenizer(pair[1]):\n",
        "            E_maxlength += 1\n",
        "        max_len = max(max_len, E_maxlength, H_maxlength)\n",
        "    max_length_train[batch_id] = max_len + 2\n",
        "    batch_id += 1\n",
        "\n",
        "max_length_test = {}  # stores maximum length of sentences in test/validation data\n",
        "batch_id = 1\n",
        "# computing maximum length for each batch of validation data\n",
        "for i in range(0, len(val_ds), batch_size):\n",
        "    max_len = 0\n",
        "    for pair in val_ds[i : i + batch_size]:\n",
        "        E_maxlength = 0\n",
        "        H_maxlength = 0\n",
        "        for token in hindi_tokenizer(pair[0]):\n",
        "            H_maxlength += 1\n",
        "        for token in english_tokenizer(pair[1]):\n",
        "            E_maxlength += 1\n",
        "        max_len = max(max_len, E_maxlength, H_maxlength)\n",
        "    max_length_test[batch_id] = max_len + 2\n",
        "    batch_id += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvWA6jNEqF_W"
      },
      "outputs": [],
      "source": [
        "# H_sentenceToTensor function takes a sentence, maximum length as argument and returns a tensor of indices with padding done, if required.\n",
        "def H_sentenceToTensor(sentence, max_length):\n",
        "    # append start of sequence token at beginning\n",
        "    src_index = [H_word2index[\"<sos>\"]]\n",
        "    for token in hindi_tokenizer(sentence):\n",
        "        # if the word in not present in dictionary then index corresponding to unknown token '<unk>' i.e. 2 is used\n",
        "        src_index.append(H_word2index.get(token, 2))\n",
        "    # append end of sequence token\n",
        "    src_index.append(H_word2index[\"<eos>\"])\n",
        "    # check if length of sentence is less than maximum length, if yes, then append <pad> token\n",
        "    if len(src_index) < max_length:\n",
        "        while len(src_index) != max_length:\n",
        "            src_index.append(H_word2index[\"<pad>\"])\n",
        "    return torch.Tensor(\n",
        "        src_index\n",
        "    )  # returning tensor of indices with length equal to max_length\n",
        "\n",
        "\n",
        "# H_sentenceToTensor function takes a sentence, maximum length as argument and returns a tensor of indices with padding done, if required.\n",
        "def E_sentenceToTensor(sentence, max_length):\n",
        "    # append start of sequence token at beginning\n",
        "    trg_index = [E_word2index[\"<sos>\"]]\n",
        "    for token in english_tokenizer(sentence):\n",
        "        # if the word in not present in dictionary then index corresponding to unknown token '<unk>' i.e. 2 is used\n",
        "        trg_index.append(E_word2index.get(token, 2))\n",
        "    # append end of sequence token\n",
        "    trg_index.append(E_word2index[\"<eos>\"])\n",
        "    # check if length of sentence is less than maximum length, if yes, then append <pad> token\n",
        "    if len(trg_index) < max_length:\n",
        "        while len(trg_index) != max_length:\n",
        "            trg_index.append(E_word2index[\"<pad>\"])\n",
        "    return torch.Tensor(\n",
        "        trg_index\n",
        "    )  # returning tensor of indices with length equal to max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxSftaT2fVD-"
      },
      "outputs": [],
      "source": [
        "train_tensor = []  # stores tensor of indexes of training data\n",
        "test_tensor = []  # stores tensor of indexes of validation/test data\n",
        "\n",
        "# finding tensor of indexes of training data\n",
        "batch_id = 1\n",
        "for i in range(0, len(train_ds), batch_size):\n",
        "    max_len = max_length_train[batch_id]\n",
        "    for pair in train_ds[i : i + batch_size]:\n",
        "        source_tensor = H_sentenceToTensor(pair[0], max_len)\n",
        "        target_tensor = E_sentenceToTensor(pair[1], max_len)\n",
        "        train_tensor.append([source_tensor, target_tensor])\n",
        "    batch_id += 1\n",
        "\n",
        "# finding tensor of indexes of validation/test data\n",
        "batch_id = 1\n",
        "for i in range(0, len(val_ds), batch_size):\n",
        "    max_len = max_length_test[batch_id]\n",
        "    for pair in val_ds[i : i + batch_size]:\n",
        "        source_tensor = H_sentenceToTensor(pair[0], max_len)\n",
        "        target_tensor = E_sentenceToTensor(pair[1], max_len)\n",
        "        test_tensor.append([source_tensor, target_tensor])\n",
        "    batch_id += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3biKde5Uoqf"
      },
      "outputs": [],
      "source": [
        "# finding train and test iterator using data loader\n",
        "# shuffle=false is used so that data remains sorted in batches\n",
        "train_iterator = DataLoader(train_tensor, batch_size=batch_size, shuffle=False)\n",
        "test_iterator = DataLoader(test_tensor, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWdhymbD0aGu"
      },
      "outputs": [],
      "source": [
        "# function to evaluate the validation loss in each epoch\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y) in enumerate(iterator):\n",
        "            input_sentence = x.long()\n",
        "            target_sentence = y.long()\n",
        "            # input_sentence and target_sentence have shape = (batch_size, maximum length) but we need shape to be (maximum length, batch_size ) so they are transposed\n",
        "            input_sentence = torch.transpose(input_sentence, 0, 1).to(device)\n",
        "            target_sentence = torch.transpose(target_sentence, 0, 1).to(device)\n",
        "\n",
        "            output = model(\n",
        "                input_sentence, target_sentence, 0\n",
        "            )  # turn off teacher forcing\n",
        "            output_dim = output.shape[2]\n",
        "            output = output[1:].reshape(-1, output_dim)\n",
        "\n",
        "            target_sentence = target_sentence[1:].reshape(-1)\n",
        "            loss = criterion(output, target_sentence)\n",
        "            epoch_loss += loss.item()\n",
        "            del target_sentence, output, input_sentence\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9P9aUnZFBux"
      },
      "outputs": [],
      "source": [
        "# defining path to store the model in different epochs\n",
        "path = \"final_phase.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgVdm_dLtspZ"
      },
      "outputs": [],
      "source": [
        "attention = Attention(hidden_size)\n",
        "encoder = Encoder(input_size, embedding_size, hidden_size, dropout).to(device)\n",
        "decoder = Decoder(attention, embedding_size, hidden_size, output_size, dropout).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "model = seq2seq(encoder, decoder, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6aoz8ESw_xE"
      },
      "outputs": [],
      "source": [
        "pad_index = E_word2index[\n",
        "    \"<pad>\"\n",
        "]  # finding the index of token <pad> in english vocabulary\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    ignore_index=pad_index\n",
        ")  # padding token is being ignored while loss computation because we don't want to pay price for <pad> token\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)  # AdamW optimizer is used\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d9OSyYSKpk7"
      },
      "source": [
        "Initialising the weights of the model using Normal distribution with mean 0 and standard deviation 0.01.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUzMf8ZILiqb",
        "outputId": "8fea2c51-37e5-4ebc-ed5f-76b3ad164e16"
      },
      "outputs": [],
      "source": [
        "def init_weights(model):\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            nn.init.normal_(parameter.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(parameter.data, 0)\n",
        "\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2YoH5S_q_NX"
      },
      "source": [
        "The model is stored after every 5 epochs and then prediction on validation set is generated from these models. The model which gave the best score using the provided evaluation script is submitted. The model was run for 35 epochs by training in two parts (due to colab runtime limitations). Best score is obtained by training the model for 25 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh_LXPFTfD1H",
        "outputId": "1de9a1a0-a56c-41dc-9f92-9aa6bd473136"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    print(f\"[Epoch {epoch} / {epochs}]\")\n",
        "    model.eval()\n",
        "    model.train()\n",
        "    i = 0\n",
        "    for id, (x, y) in enumerate(\n",
        "        train_iterator\n",
        "    ):  # iterating over batches of train_iterator\n",
        "        input_sentence = x.long()\n",
        "        target_sentence = y.long()\n",
        "\n",
        "        # input_sentence and target_sentence have shape = (batch_size, maximum length) but we need shape to be (maximum length, batch_size ) so they are transposed\n",
        "        input_sentence = torch.transpose(input_sentence, 0, 1).to(device)\n",
        "        target_sentence = torch.transpose(target_sentence, 0, 1).to(device)\n",
        "\n",
        "        output = model(input_sentence, target_sentence)  # forward propagation\n",
        "        output = output[1:].view(\n",
        "            -1, output.shape[-1]\n",
        "        )  # removing the start token from model's prediction and reshaping it to make it make it fit for input to loss function\n",
        "\n",
        "        target_sentence = target_sentence[1:].reshape(\n",
        "            -1\n",
        "        )  # removing the start token from actual target translation\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target_sentence)\n",
        "\n",
        "        loss.backward()  # backward propagation\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            model.parameters(), max_norm=1\n",
        "        )  # clipping the gradients to keep them in reasonable range\n",
        "        optimizer.step()  # gradient descent. The optimizer iterates over all parameters (tensors) to be updated and their internally stored gradients are used.\n",
        "        del target_sentence, output, input_sentence\n",
        "        step += 1\n",
        "        epoch_loss += loss.item()  # adding the epoch loss for each batch\n",
        "    if epoch % 5 == 0:  # saving the model after every 5 epochs\n",
        "        torch.save(model, path)\n",
        "    val_loss = evaluate(model, test_iterator, criterion)\n",
        "    print(\"Train loss : \", epoch_loss / len(train_iterator))\n",
        "    print(\"Validation loss : \", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUzX_lk0CZ2q",
        "outputId": "0020155f-c408-4184-fc25-8bbe0153fa40"
      },
      "outputs": [],
      "source": [
        "torch.save(model, path)  # saving the trained model at defined location\n",
        "# model.train()\n",
        "# model = torch.load(path) #loading the model\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP8DEIb7-O3-"
      },
      "source": [
        "Function to translate hindi sentences(index vectors) to english sentences(index vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLiRpO4xjGb8"
      },
      "outputs": [],
      "source": [
        "def hin_to_eng_translation(model, device, hindi_num_vec, max_length=70):\n",
        "    hindi_tensor = torch.LongTensor(hindi_num_vec).unsqueeze(1).to(device)\n",
        "    with torch.no_grad():\n",
        "        encoder_states, hidden = model.encoder(hindi_tensor)\n",
        "    eng_num_vec = [E_word2index[\"<sos>\"]]  # adding index for <sos> token\n",
        "    eos_idx = E_word2index[\"<eos>\"]  # adding index for <eos> token\n",
        "    for _ in range(max_length):\n",
        "        curr_input = torch.LongTensor([eng_num_vec[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(curr_input, hidden, encoder_states)\n",
        "            curr_output = output.argmax(1).item()\n",
        "        eng_num_vec.append(\n",
        "            curr_output\n",
        "        )  # appending the prediction in english index vector\n",
        "        if (\n",
        "            curr_output == eos_idx\n",
        "        ):  # stop generating predictions once eos token is encountered\n",
        "            break\n",
        "    return eng_num_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQJY-N_M--Rl"
      },
      "source": [
        "# 7. Testing the Model\n",
        "\n",
        "Obtaining the reference and prediction files to compute Bleu score and Meteor Score using Evaluation Script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsVVUsiQ-NXJ"
      },
      "source": [
        "Obtaining the predicted sentences for validation set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHGXLM_PfLqC"
      },
      "outputs": [],
      "source": [
        "file1 = open(\n",
        "    \"validation_prediction.txt\", \"w\"\n",
        ")  # to store the prediction of validation set\n",
        "csv_file = open(\"validation_ds.csv\", encoding=\"utf-8\")\n",
        "rows = csv.reader(csv_file)\n",
        "for row in rows:\n",
        "    hindi_sentence = row[0]\n",
        "    hindi_sentence_token = []\n",
        "\n",
        "    # tokenize hindi sentence\n",
        "    if type(hindi_sentence) == str:\n",
        "        for t in hindi_tokenizer(hindi_sentence):\n",
        "            hindi_sentence_token.append(t)\n",
        "    else:\n",
        "        for t in hindi_sentence:\n",
        "            hindi_sentence_token.append(t)\n",
        "\n",
        "    hindi_sentence_token.insert(0, \"<sos>\")  # append <sos> token\n",
        "    hindi_sentence_token.append(\"<eos>\")  # append <eos> token\n",
        "    hindi_num_vec = []\n",
        "\n",
        "    # generating index vector for hindi sentences\n",
        "    for t in hindi_sentence_token:\n",
        "        hindi_num_vec.append(H_word2index.get(t, 2))\n",
        "\n",
        "    # call hin_to_eng_translation function to generate predictions\n",
        "    eng_num_vec = hin_to_eng_translation(model, device, hindi_num_vec, max_length=70)\n",
        "    # eng_num_vec is vector of indices of predicted english sentences. Now, we need to find the words corresponding to these indices\n",
        "\n",
        "    english_sentence_list = []\n",
        "    for word_idx in eng_num_vec:\n",
        "        english_sentence_list.append(\n",
        "            E_index2word.get(word_idx, 2)\n",
        "        )  # index 2 is for <unk>.\n",
        "\n",
        "    english_sentence_list.pop(0)  # remove <sos> token\n",
        "    english_sentence_list.pop()  # remove <eos> token\n",
        "\n",
        "    # storing the sentences in form of string (while prediction these words were stored in list that's why now there is need to store them as string)\n",
        "    english_sentence = \"\"\n",
        "    if len(english_sentence_list) > 0:\n",
        "        for string in english_sentence_list[0:]:\n",
        "            english_sentence += string + \" \"\n",
        "    file1.write(english_sentence + \"\\n\")\n",
        "\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP0scsSD-veO"
      },
      "source": [
        "Saving the refernce sentences for validation set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvZ_2KS6R-6A"
      },
      "outputs": [],
      "source": [
        "csv_file = open(\"validation_ds.csv\", encoding=\"utf-8\")\n",
        "rows = csv.reader(csv_file)\n",
        "file = open(\"validation_english.txt\", \"w\")\n",
        "for row in rows:\n",
        "    file.write(row[1] + \"\\n\")\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5FL52a3lSeB"
      },
      "source": [
        "# 8. Generating Predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm9H6Xuc_YEa"
      },
      "source": [
        "### Obtaining translation for testhindistatements.csv\n",
        "\n",
        "To obtain prediction for the test statements, first the hindi statements are cleaned in the same way as the train set was processed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uimswS_sjXQS",
        "outputId": "df9c58a9-d48b-4294-b85d-77858acf3c41"
      },
      "outputs": [],
      "source": [
        "prediction = []  # contains statements for which prediction is to be generated\n",
        "\n",
        "# reading the testhindistatements.csv file\n",
        "csv_file = open(\"testhindistatements.csv\", encoding=\"utf-8\")\n",
        "rows = csv.reader(csv_file)\n",
        "for row in rows:\n",
        "    prediction.append(row[2])\n",
        "\n",
        "prediction = prediction[1:]  # removing the column name\n",
        "prediction[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10GrsgkkkF96",
        "outputId": "736bdf8c-a96c-48f6-8ffb-56e5823539f4"
      },
      "outputs": [],
      "source": [
        "# pre-processing of hindi statements is carried out\n",
        "for i in range(0, len(prediction)):\n",
        "    for (\n",
        "        src,\n",
        "        trg,\n",
        "    ) in (\n",
        "        processing_dict.items()\n",
        "    ):  # processing_dict is defined in Section 3: Pre-processing data\n",
        "        if src in hindi_tokenizer(prediction[i]):\n",
        "            prediction[i] = prediction[i].replace(src, trg)\n",
        "    prediction[i] = prediction[i].replace(\"...\", \" \")\n",
        "    prediction[i] = prediction[i].replace(\".\", \"|\")\n",
        "prediction[0:10]  # cleaned hindi sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DbsiB8s79K4"
      },
      "outputs": [],
      "source": [
        "file1 = open(\"answer.txt\", \"w\")  # file to store the predictions\n",
        "\n",
        "for hindi_sentence in prediction:\n",
        "    hindi_sentence_token = []\n",
        "    # tokenize hindi sentence\n",
        "    if type(hindi_sentence) == str:\n",
        "        for t in indic_tokenize.trivial_tokenize(hindi_sentence):\n",
        "            hindi_sentence_token.append(t)\n",
        "    else:\n",
        "        for t in hindi_sentence:\n",
        "            hindi_sentence_token.append(t)\n",
        "\n",
        "    hindi_sentence_token.insert(0, \"<sos>\")  # append <sos> token\n",
        "    hindi_sentence_token.append(\"<eos>\")  # append <eos> token\n",
        "    hindi_num_vec = []\n",
        "\n",
        "    # generating index vector for hindi sentences\n",
        "    for t in hindi_sentence_token:\n",
        "        hindi_num_vec.append(H_word2index.get(t, 2))  # index 2 is for <unk> token\n",
        "\n",
        "    # call hin_to_eng_translation function to generate predictions\n",
        "    eng_num_vec = hin_to_eng_translation(model, device, hindi_num_vec, max_length=70)\n",
        "\n",
        "    # eng_num_vec is vector of indices of predicted english sentences. Now, we need to find the words corresponding to these indices\n",
        "\n",
        "    english_sentence_list = []\n",
        "    for word_idx in eng_num_vec:\n",
        "        english_sentence_list.append(\n",
        "            E_index2word.get(word_idx, 2)\n",
        "        )  # index 2 is for <unk>.\n",
        "    # english_sentence_list contains predicted english words. Now, we need to obtain the sentences as string from this list of words\n",
        "\n",
        "    english_sentence_list.pop(0)  # remove <sos> token\n",
        "    english_sentence_list.pop()  # remove <eos> token\n",
        "\n",
        "    # storing the sentences in form of string (while prediction these words were stored in list that's why now there is need to store them as string)\n",
        "    english_sentence = \"\"\n",
        "    if len(english_sentence_list) > 0:\n",
        "        for string in english_sentence_list[0:]:\n",
        "            english_sentence += string + \" \"\n",
        "    file1.write(english_sentence + \"\\n\")\n",
        "\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEKBy46fsxda"
      },
      "source": [
        "# 9. References\n",
        "\n",
        "[1] [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)\n",
        "\n",
        "[2] [https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "NMT_Final_Phase",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
